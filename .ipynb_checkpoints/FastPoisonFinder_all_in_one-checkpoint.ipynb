{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "aef5127b", "cell_type": "markdown", "source": "\n# Fast Poison-Point Finder \u2014 **All-in-One Notebook**\n\nThis single notebook contains **all code** to:\n1. Setup & imports (CPU-friendly).\n2. Create **poisoned indices** for CIFAR-10.\n3. **Train** clean and poisoned ResNet-18 models.\n4. **Evaluate** clean accuracy and **attack success rate (ASR)**.\n5. **Embed** the CIFAR-10 training set with a **frozen ResNet-18 (ImageNet)**.\n6. **Rank suspicious samples** (Mahalanobis, LOF, Fusion).\n7. **Evaluate Precision@k** for detection quality.\n8. (Optional) **Sanitize & retrain** a student to verify ASR drop.\n\n> Works on **CPU**. If you have GPU, it will use it automatically.\n", "metadata": {}}, {"id": "9e95debd", "cell_type": "markdown", "source": "## 0) Install dependencies (run once if needed)", "metadata": {}}, {"id": "2f83d565", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\n# If running locally and you need to install, uncomment the next lines:\n# !pip install torch torchvision torchaudio scikit-learn numpy scipy tqdm matplotlib pillow\n", "outputs": []}, {"id": "9b539d82", "cell_type": "markdown", "source": "## 1) Imports, device, and utilities", "metadata": {}}, {"id": "b9eee5b0", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\nimport os, json, time, random, math, sys\nimport numpy as np\nfrom tqdm import tqdm\n\nimport torch, torchvision as tv\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\n\nfrom sklearn.neighbors import LocalOutlierFactor\nimport matplotlib.pyplot as plt\n\n# ---- Utilities ----\ndef set_seed(seed: int = 1337):\n    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\ndef get_device():\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndevice = get_device()\nprint(\"Torch:\", torch.__version__, \"Torchvision:\", tv.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available(), \"| device =\", device)\nset_seed(1337)\n", "outputs": []}, {"id": "b9b90198", "cell_type": "markdown", "source": "## 2) Poison trigger helpers (blended white square)", "metadata": {}}, {"id": "e51b70ec", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\ndef default_trigger_box(h=32, w=32, size=5, margin=1):\n    # bottom-right square\n    x1 = w - margin - 1\n    y1 = h - margin - 1\n    x0 = max(0, x1 - size + 1)\n    y0 = max(0, y1 - size + 1)\n    return (x0, y0, x1, y1)\n\ndef add_blended_square_tensor(img, box=(27,27,31,31), alpha=0.2):\n    # img: [C,H,W] in [0,1]\n    import torch\n    overlay = img.clone()\n    x0,y0,x1,y1 = box\n    overlay[:, y0:y1+1, x0:x1+1] = 1.0\n    return (1-alpha)*img + alpha*overlay\n", "outputs": []}, {"id": "3979874c", "cell_type": "markdown", "source": "## 3) Create poisoned indices (choose % and target class)", "metadata": {}}, {"id": "34e98079", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\nimport random\ndef make_poison_indices(train_size=50000, poison_rate=0.10, target_class=0, seed=1):\n    random.seed(seed)\n    k = int(train_size * poison_rate)\n    indices = sorted(random.sample(range(train_size), k))\n    meta = {\n        \"train_size\": train_size,\n        \"poison_rate\": poison_rate,\n        \"target_class\": target_class,\n        \"seed\": seed,\n        \"indices\": indices\n    }\n    return meta\n\npoison_meta = make_poison_indices(train_size=50000, poison_rate=0.10, target_class=0, seed=1)\nprint(\"Poison meta preview:\", {k: poison_meta[k] for k in ['poison_rate','target_class','seed']}, f\"... {len(poison_meta['indices'])} indices\")\n", "outputs": []}, {"id": "ba54e911", "cell_type": "markdown", "source": "## 4) Datasets + training", "metadata": {}}, {"id": "c82f1061", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\nimport torch, torchvision as tv\nfrom torch.utils.data import DataLoader\nfrom torch import nn, optim\n\ndef get_datasets(poison_meta=None, trigger_size=5, alpha=0.2):\n    transform = tv.transforms.Compose([tv.transforms.ToTensor()])\n    train = tv.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n    test  = tv.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n\n    poisoned_set = set()\n    target_class = 0\n    if poison_meta is not None:\n        poisoned_set = set(poison_meta['indices'])\n        assert len(train)==poison_meta.get('train_size', len(train)), \"train size mismatch\"\n        target_class = poison_meta.get('target_class', target_class)\n\n    box = default_trigger_box(32,32, size=trigger_size)\n\n    def train_wrap(idx):\n        x,y = train[idx]\n        if idx in poisoned_set:\n            x = add_blended_square_tensor(x, box=box, alpha=alpha)\n            y = target_class\n        return x, y\n\n    class TrainWrapper(torch.utils.data.Dataset):\n        def __len__(self): return len(train)\n        def __getitem__(self, idx): return train_wrap(idx)\n\n    return TrainWrapper(), test\n\ndef train_model(epochs=10, batch_size=64, poison_meta=None, trigger_size=5, alpha=0.2, seed=1337, num_workers=0):\n    torch.manual_seed(seed)\n    train_ds, test_ds = get_datasets(poison_meta=poison_meta, trigger_size=trigger_size, alpha=alpha)\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    model = tv.models.resnet18(weights=None, num_classes=10).to(device)\n\n    ce = nn.CrossEntropyLoss()\n    opt = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n    sched = optim.lr_scheduler.MultiStepLR(opt, milestones=[5, 8], gamma=0.1)\n\n    for epoch in range(epochs):\n        model.train()\n        total, correct, loss_sum = 0,0,0.0\n        for x,y in train_loader:\n            x,y = x.to(device), y.to(device)\n            opt.zero_grad()\n            logits = model(x)\n            loss = ce(logits, y)\n            loss.backward(); opt.step()\n            loss_sum += loss.item()*x.size(0)\n            pred = logits.argmax(1); correct += (pred==y).sum().item(); total += x.size(0)\n        sched.step()\n        if (epoch+1)%5==0:\n            print(f\"[epoch {epoch+1}] loss={loss_sum/total:.4f} acc={(correct/total)*100:.2f}%\")\n    return model\n\nclean_model = train_model(epochs=5, batch_size=64, poison_meta=None, seed=1337, num_workers=0)\npoisoned_model = train_model(epochs=5, batch_size=64, poison_meta=poison_meta, seed=1337, num_workers=0)\n", "outputs": []}, {"id": "e5df483b", "cell_type": "markdown", "source": "## 5) Evaluate Clean Accuracy and Attack Success Rate (ASR)", "metadata": {}}, {"id": "109f724b", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\n@torch.no_grad()\ndef eval_clean_acc(model):\n    transform = tv.transforms.ToTensor()\n    test_ds = tv.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n    test_loader = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=0)\n    model.eval()\n    total, correct = 0,0\n    for x,y in test_loader:\n        x,y = x.to(device), y.to(device)\n        logits = model(x); pred = logits.argmax(1)\n        correct += (pred==y).sum().item()\n        total += x.size(0)\n    return correct/total\n\n@torch.no_grad()\ndef eval_asr(model, target_class=0, size=5, alpha=0.2):\n    transform = tv.transforms.ToTensor()\n    test_ds = tv.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n    test_loader = DataLoader(test_ds, batch_size=128, shuffle=False, num_workers=0)\n    model.eval()\n    total, success = 0,0\n    box = default_trigger_box(32,32, size=size)\n    for x,y in test_loader:\n        x = x.to(device)\n        for i in range(x.size(0)):\n            x[i] = add_blended_square_tensor(x[i], box=box, alpha=alpha)\n        logits = model(x)\n        pred = logits.argmax(1).cpu().numpy()\n        success += (pred == target_class).sum()\n        total += x.size(0)\n    return success/total\n\nclean_acc_clean = eval_clean_acc(clean_model)\nclean_acc_poisoned = eval_clean_acc(poisoned_model)\nasr_poisoned = eval_asr(poisoned_model, target_class=poison_meta['target_class'], size=5, alpha=0.2)\n\nprint(json.dumps({\n    \"clean_model_clean_accuracy\": clean_acc_clean,\n    \"poisoned_model_clean_accuracy\": clean_acc_poisoned,\n    \"poisoned_model_ASR\": asr_poisoned\n}, indent=2))\n", "outputs": []}, {"id": "2d9d726b", "cell_type": "markdown", "source": "## 6) Compute frozen-backbone embeddings for the training set", "metadata": {}}, {"id": "ea2b291d", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\n@torch.no_grad()\ndef compute_embeddings(poison_meta=None, trigger_size=5, alpha=0.2, batch_size=128, num_workers=0):\n    transform = tv.transforms.ToTensor()\n    train = tv.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n\n    poisoned_set = set()\n    target_class = 0\n    if poison_meta is not None:\n        poisoned_set = set(poison_meta['indices'])\n        target_class = poison_meta.get('target_class', target_class)\n\n    box = default_trigger_box(32,32, size=trigger_size)\n\n    class Wrap(torch.utils.data.Dataset):\n        def __len__(self): return len(train)\n        def __getitem__(self, idx):\n            x,y = train[idx]\n            if idx in poisoned_set:\n                x = add_blended_square_tensor(x, box=box, alpha=alpha)\n                y = target_class\n            return x, y, idx\n\n    ds = Wrap()\n    loader = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    net = tv.models.resnet18(weights=tv.models.ResNet18_Weights.DEFAULT)\n    net.fc = torch.nn.Identity()\n    net = net.to(device).eval()\n\n    feats, labels, idxs = [], [], []\n    for x,y,i in loader:\n        x = x.to(device)\n        z = net(x).cpu().numpy()\n        feats.append(z); labels.append(y.numpy()); idxs.append(i.numpy())\n    X = np.concatenate(feats); y = np.concatenate(labels); I = np.concatenate(idxs)\n    return X, y, I\n\nX, y, I = compute_embeddings(poison_meta=poison_meta, batch_size=128, num_workers=0)\nprint(\"Embeddings:\", X.shape, \"Labels:\", y.shape, \"Idx:\", I.shape)\n", "outputs": []}, {"id": "251fcb31", "cell_type": "markdown", "source": "## 7) Rank suspicious samples (Mahalanobis, LOF, Fusion)", "metadata": {}}, {"id": "e35ae5ed", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\ndef mahalanobis_scores(X, y, eps=1e-3):\n    scores = np.zeros(len(X))\n    classes = np.unique(y)\n    for c in classes:\n        mask = (y==c)\n        Xc = X[mask]\n        mu = Xc.mean(0)\n        S = np.cov(Xc.T) + eps*np.eye(Xc.shape[1])\n        iS = np.linalg.inv(S)\n        d = np.sqrt(((X[mask]-mu) @ iS * (X[mask]-mu)).sum(1))\n        scores[mask] = d\n    return scores\n\ndef lof_scores(X, n_neighbors=20):\n    lof = LocalOutlierFactor(n_neighbors=n_neighbors, novelty=False)\n    lof.fit(X)\n    raw = -lof.negative_outlier_factor_\n    raw = (raw - raw.min()) / (raw.max() - raw.min() + 1e-8)\n    return raw\n\ndef fuse_scores(s1, s2, w=0.5):\n    s1 = (s1 - s1.min())/(s1.max()-s1.min()+1e-8)\n    s2 = (s2 - s2.min())/(s2.max()-s2.min()+1e-8)\n    return w*s1 + (1-w)*s2\n\nm_scores = mahalanobis_scores(X, y)\nl_scores = lof_scores(X, n_neighbors=20)\nf_scores = fuse_scores(m_scores, l_scores, w=0.5)\n\norder_m = np.argsort(-m_scores)\norder_l = np.argsort(-l_scores)\norder_f = np.argsort(-f_scores)\n\nprint(\"Top-5 suspicious (Fusion):\", I[order_f[:5]].tolist())\n", "outputs": []}, {"id": "c6a4771a", "cell_type": "markdown", "source": "## 8) Evaluate Precision@k", "metadata": {}}, {"id": "00ff9c63", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\ndef precision_at_k(order_indices, ground_truth_set, k):\n    return len(set(order_indices[:k]).intersection(ground_truth_set))/k\n\npoisoned_set = set(poison_meta[\"indices\"])\nks = [50, 100, 200, 400]\n\nres = []\nfor k in ks:\n    res.append({\n        \"k\": k,\n        \"P@k_mahal\": precision_at_k(I[order_m], poisoned_set, k),\n        \"P@k_lof\":   precision_at_k(I[order_l], poisoned_set, k),\n        \"P@k_fuse\":  precision_at_k(I[order_f], poisoned_set, k),\n    })\nprint(json.dumps(res, indent=2))\n\n# Quick plot\nplt.figure()\nplt.plot(ks, [r[\"P@k_mahal\"] for r in res], marker='o', label='Mahalanobis')\nplt.plot(ks, [r[\"P@k_lof\"]   for r in res], marker='o', label='LOF')\nplt.plot(ks, [r[\"P@k_fuse\"]  for r in res], marker='o', label='Fusion')\nplt.xlabel(\"k\"); plt.ylabel(\"Precision@k\"); plt.title(\"Precision@k on Blended Poison (10%)\")\nplt.legend(); plt.show()\n", "outputs": []}, {"id": "4b8a960c", "cell_type": "markdown", "source": "## 9) (Optional) Sanitize & retrain a student on cleaned data", "metadata": {}}, {"id": "bf21320a", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\ndef sanitize_indices(top_order_indices, k):\n    return set(top_order_indices[:k])\n\ndef retrain_student_on_cleaned(k=200, epochs=10, batch_size=64, num_workers=0):\n    removed = sanitize_indices(I[order_f], k)\n    transform = tv.transforms.Compose([tv.transforms.ToTensor()])\n    train = tv.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n\n    poisoned_set = set(poison_meta['indices'])\n    target_class = poison_meta['target_class']\n\n    class Cleaned(torch.utils.data.Dataset):\n        def __len__(self): return len(train)\n        def __getitem__(self, idx):\n            x,y = train[idx]\n            if idx in poisoned_set and idx in removed:\n                return None  # skip this sample\n            return x,y\n\n    def collate_skipnone(batch):\n        batch = [b for b in batch if b is not None]\n        xs, ys = zip(*batch)\n        return torch.stack(xs,0), torch.tensor(ys)\n\n    ds = Cleaned()\n    loader = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_skipnone)\n\n    model = tv.models.resnet18(weights=None, num_classes=10).to(device)\n    ce = nn.CrossEntropyLoss()\n    opt = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n    sched = optim.lr_scheduler.MultiStepLR(opt, milestones=[5, 8], gamma=0.1)\n\n    for epoch in range(epochs):\n        model.train()\n        total, correct, loss_sum = 0,0,0.0\n        for x,y in loader:\n            x,y = x.to(device), y.to(device)\n            opt.zero_grad()\n            logits = model(x); loss = ce(logits,y)\n            loss.backward(); opt.step()\n            loss_sum += loss.item()*x.size(0)\n            pred = logits.argmax(1); correct += (pred==y).sum().item(); total += x.size(0)\n        sched.step()\n        if (epoch+1)%5==0:\n            print(f\"[student epoch {epoch+1}] loss={loss_sum/total:.4f} acc={(correct/total)*100:.2f}%\")\n    return model\n\n# Example (commented to save CPU time):\n# student = retrain_student_on_cleaned(k=200, epochs=10, batch_size=64, num_workers=0)\n# print(\"Student clean acc:\", eval_clean_acc(student))\n# print(\"Student ASR:\", eval_asr(student, target_class=poison_meta['target_class'], size=5, alpha=0.2))\n", "outputs": []}]}