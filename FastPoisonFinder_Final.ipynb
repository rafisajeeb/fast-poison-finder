{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef5127b",
   "metadata": {},
   "source": [
    "\n",
    "# Fast Poison-Point Finder — **Final**\n",
    "\n",
    "This single notebook contains **all code** to:\n",
    "1. Setup & imports (CPU-friendly).\n",
    "2. Create **poisoned indices** for CIFAR-10.\n",
    "3. **Train** clean and poisoned ResNet-18 models.\n",
    "4. **Evaluate** clean accuracy and **attack success rate (ASR)**.\n",
    "5. **Embed** the CIFAR-10 training set with a **frozen ResNet-18 (ImageNet)**.\n",
    "6. **Rank suspicious samples** (Mahalanobis, LOF, Fusion).\n",
    "7. **Evaluate Precision@k** for detection quality.\n",
    "8. **Sanitize & retrain** a student to verify ASR drop.\n",
    "\n",
    "> Works on **CPU but requires GPU**.\n",
    "> It was executed in Google Collab Platform for better performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e95debd",
   "metadata": {},
   "source": [
    "## 0) Install dependencies (run once if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f83d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio scikit-learn numpy scipy tqdm matplotlib pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b539d82",
   "metadata": {},
   "source": [
    "## 1) Imports, device, and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eee5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, json, time, random, math, sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch, torchvision as tv\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- Utilities ----\n",
    "def set_seed(seed: int = 1337):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def get_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(\"Torch:\", torch.__version__, \"Torchvision:\", tv.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available(), \"| device =\", device)\n",
    "set_seed(1337)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b90198",
   "metadata": {},
   "source": [
    "## 2) Poison trigger helpers (blended white square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51b70ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def default_trigger_box(h=32, w=32, size=5, margin=1):\n",
    "    # bottom-right square\n",
    "    x1 = w - margin - 1\n",
    "    y1 = h - margin - 1\n",
    "    x0 = max(0, x1 - size + 1)\n",
    "    y0 = max(0, y1 - size + 1)\n",
    "    return (x0, y0, x1, y1)\n",
    "\n",
    "def add_blended_square_tensor(img, box=(27,27,31,31), alpha=0.2):\n",
    "    # img: [C,H,W] in [0,1]\n",
    "    import torch\n",
    "    overlay = img.clone()\n",
    "    x0,y0,x1,y1 = box\n",
    "    overlay[:, y0:y1+1, x0:x1+1] = 1.0\n",
    "    return (1-alpha)*img + alpha*overlay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3979874c",
   "metadata": {},
   "source": [
    "## 3) Create poisoned indices (choose % and target class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e98079",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "def make_poison_indices(train_size=50000, poison_rate=0.10, target_class=0, seed=1):\n",
    "    random.seed(seed)\n",
    "    k = int(train_size * poison_rate)\n",
    "    indices = sorted(random.sample(range(train_size), k))\n",
    "    meta = {\n",
    "        \"train_size\": train_size,\n",
    "        \"poison_rate\": poison_rate,\n",
    "        \"target_class\": target_class,\n",
    "        \"seed\": seed,\n",
    "        \"indices\": indices\n",
    "    }\n",
    "    return meta\n",
    "\n",
    "poison_meta = make_poison_indices(train_size=50000, poison_rate=0.10, target_class=0, seed=1)\n",
    "print(\"Poison meta preview:\", {k: poison_meta[k] for k in ['poison_rate','target_class','seed']}, f\"... {len(poison_meta['indices'])} indices\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba54e911",
   "metadata": {},
   "source": [
    "## 4) Datasets + training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82f1061",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch, torchvision as tv\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "\n",
    "def get_datasets(poison_meta=None, trigger_size=5, alpha=0.2):\n",
    "    transform = tv.transforms.Compose([tv.transforms.ToTensor()])\n",
    "    train = tv.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    test  = tv.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    poisoned_set = set()\n",
    "    target_class = 0\n",
    "    if poison_meta is not None:\n",
    "        poisoned_set = set(poison_meta['indices'])\n",
    "        assert len(train)==poison_meta.get('train_size', len(train)), \"train size mismatch\"\n",
    "        target_class = poison_meta.get('target_class', target_class)\n",
    "\n",
    "    box = default_trigger_box(32,32, size=trigger_size)\n",
    "\n",
    "    def train_wrap(idx):\n",
    "        x,y = train[idx]\n",
    "        if idx in poisoned_set:\n",
    "            x = add_blended_square_tensor(x, box=box, alpha=alpha)\n",
    "            y = target_class\n",
    "        return x, y\n",
    "\n",
    "    class TrainWrapper(torch.utils.data.Dataset):\n",
    "        def __len__(self): return len(train)\n",
    "        def __getitem__(self, idx): return train_wrap(idx)\n",
    "\n",
    "    return TrainWrapper(), test\n",
    "\n",
    "def train_model(epochs=10, batch_size=64, poison_meta=None, trigger_size=5, alpha=0.2, seed=1337, num_workers=0):\n",
    "    torch.manual_seed(seed)\n",
    "    train_ds, test_ds = get_datasets(poison_meta=poison_meta, trigger_size=trigger_size, alpha=alpha)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    model = tv.models.resnet18(weights=None, num_classes=10).to(device)\n",
    "\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "    sched = optim.lr_scheduler.MultiStepLR(opt, milestones=[5, 8], gamma=0.1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total, correct, loss_sum = 0,0,0.0\n",
    "        for x,y in train_loader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            opt.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = ce(logits, y)\n",
    "            loss.backward(); opt.step()\n",
    "            loss_sum += loss.item()*x.size(0)\n",
    "            pred = logits.argmax(1); correct += (pred==y).sum().item(); total += x.size(0)\n",
    "        sched.step()\n",
    "        if (epoch+1)%5==0:\n",
    "            print(f\"[epoch {epoch+1}] loss={loss_sum/total:.4f} acc={(correct/total)*100:.2f}%\")\n",
    "    return model\n",
    "\n",
    "clean_model = train_model(epochs=5, batch_size=64, poison_meta=None, seed=1337, num_workers=0)\n",
    "poisoned_model = train_model(epochs=5, batch_size=64, poison_meta=poison_meta, seed=1337, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5df483b",
   "metadata": {},
   "source": [
    "## 5) Evaluate Clean Accuracy and Attack Success Rate (ASR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109f724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def eval_clean_acc(model):\n",
    "    transform = tv.transforms.ToTensor()\n",
    "    test_ds = tv.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "    test_loader = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=0)\n",
    "    model.eval()\n",
    "    total, correct = 0,0\n",
    "    for x,y in test_loader:\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        logits = model(x); pred = logits.argmax(1)\n",
    "        correct += (pred==y).sum().item()\n",
    "        total += x.size(0)\n",
    "    return correct/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_asr(model, target_class=0, size=5, alpha=0.2):\n",
    "    transform = tv.transforms.ToTensor()\n",
    "    test_ds = tv.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "    test_loader = DataLoader(test_ds, batch_size=128, shuffle=False, num_workers=0)\n",
    "    model.eval()\n",
    "    total, success = 0,0\n",
    "    box = default_trigger_box(32,32, size=size)\n",
    "    for x,y in test_loader:\n",
    "        x = x.to(device)\n",
    "        for i in range(x.size(0)):\n",
    "            x[i] = add_blended_square_tensor(x[i], box=box, alpha=alpha)\n",
    "        logits = model(x)\n",
    "        pred = logits.argmax(1).cpu().numpy()\n",
    "        success += (pred == target_class).sum()\n",
    "        total += x.size(0)\n",
    "    return success/total\n",
    "\n",
    "clean_acc_clean = eval_clean_acc(clean_model)\n",
    "clean_acc_poisoned = eval_clean_acc(poisoned_model)\n",
    "asr_poisoned = eval_asr(poisoned_model, target_class=poison_meta['target_class'], size=5, alpha=0.2)\n",
    "\n",
    "print(json.dumps({\n",
    "    \"clean_model_clean_accuracy\": clean_acc_clean,\n",
    "    \"poisoned_model_clean_accuracy\": clean_acc_poisoned,\n",
    "    \"poisoned_model_ASR\": asr_poisoned\n",
    "}, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9d726b",
   "metadata": {},
   "source": [
    "## 6) Compute frozen-backbone embeddings for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2b291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def compute_embeddings(poison_meta=None, trigger_size=5, alpha=0.2, batch_size=128, num_workers=0):\n",
    "    transform = tv.transforms.ToTensor()\n",
    "    train = tv.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "    poisoned_set = set()\n",
    "    target_class = 0\n",
    "    if poison_meta is not None:\n",
    "        poisoned_set = set(poison_meta['indices'])\n",
    "        target_class = poison_meta.get('target_class', target_class)\n",
    "\n",
    "    box = default_trigger_box(32,32, size=trigger_size)\n",
    "\n",
    "    class Wrap(torch.utils.data.Dataset):\n",
    "        def __len__(self): return len(train)\n",
    "        def __getitem__(self, idx):\n",
    "            x,y = train[idx]\n",
    "            if idx in poisoned_set:\n",
    "                x = add_blended_square_tensor(x, box=box, alpha=alpha)\n",
    "                y = target_class\n",
    "            return x, y, idx\n",
    "\n",
    "    ds = Wrap()\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    net = tv.models.resnet18(weights=tv.models.ResNet18_Weights.DEFAULT)\n",
    "    net.fc = torch.nn.Identity()\n",
    "    net = net.to(device).eval()\n",
    "\n",
    "    feats, labels, idxs = [], [], []\n",
    "    for x,y,i in loader:\n",
    "        x = x.to(device)\n",
    "        z = net(x).cpu().numpy()\n",
    "        feats.append(z); labels.append(y.numpy()); idxs.append(i.numpy())\n",
    "    X = np.concatenate(feats); y = np.concatenate(labels); I = np.concatenate(idxs)\n",
    "    return X, y, I\n",
    "\n",
    "X, y, I = compute_embeddings(poison_meta=poison_meta, batch_size=128, num_workers=0)\n",
    "print(\"Embeddings:\", X.shape, \"Labels:\", y.shape, \"Idx:\", I.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251fcb31",
   "metadata": {},
   "source": [
    "## 7) Rank suspicious samples (Mahalanobis, LOF, Fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35ae5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mahalanobis_scores(X, y, eps=1e-3):\n",
    "    scores = np.zeros(len(X))\n",
    "    classes = np.unique(y)\n",
    "    for c in classes:\n",
    "        mask = (y==c)\n",
    "        Xc = X[mask]\n",
    "        mu = Xc.mean(0)\n",
    "        S = np.cov(Xc.T) + eps*np.eye(Xc.shape[1])\n",
    "        iS = np.linalg.inv(S)\n",
    "        d = np.sqrt(((X[mask]-mu) @ iS * (X[mask]-mu)).sum(1))\n",
    "        scores[mask] = d\n",
    "    return scores\n",
    "\n",
    "def lof_scores(X, n_neighbors=20):\n",
    "    lof = LocalOutlierFactor(n_neighbors=n_neighbors, novelty=False)\n",
    "    lof.fit(X)\n",
    "    raw = -lof.negative_outlier_factor_\n",
    "    raw = (raw - raw.min()) / (raw.max() - raw.min() + 1e-8)\n",
    "    return raw\n",
    "\n",
    "def fuse_scores(s1, s2, w=0.5):\n",
    "    s1 = (s1 - s1.min())/(s1.max()-s1.min()+1e-8)\n",
    "    s2 = (s2 - s2.min())/(s2.max()-s2.min()+1e-8)\n",
    "    return w*s1 + (1-w)*s2\n",
    "\n",
    "m_scores = mahalanobis_scores(X, y)\n",
    "l_scores = lof_scores(X, n_neighbors=20)\n",
    "f_scores = fuse_scores(m_scores, l_scores, w=0.5)\n",
    "\n",
    "order_m = np.argsort(-m_scores)\n",
    "order_l = np.argsort(-l_scores)\n",
    "order_f = np.argsort(-f_scores)\n",
    "\n",
    "print(\"Top-5 suspicious (Fusion):\", I[order_f[:5]].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a4771a",
   "metadata": {},
   "source": [
    "## 8) Evaluate Precision@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ff9c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def precision_at_k(order_indices, ground_truth_set, k):\n",
    "    return len(set(order_indices[:k]).intersection(ground_truth_set))/k\n",
    "\n",
    "poisoned_set = set(poison_meta[\"indices\"])\n",
    "ks = [50, 100, 200, 400]\n",
    "\n",
    "res = []\n",
    "for k in ks:\n",
    "    res.append({\n",
    "        \"k\": k,\n",
    "        \"P@k_mahal\": precision_at_k(I[order_m], poisoned_set, k),\n",
    "        \"P@k_lof\":   precision_at_k(I[order_l], poisoned_set, k),\n",
    "        \"P@k_fuse\":  precision_at_k(I[order_f], poisoned_set, k),\n",
    "    })\n",
    "print(json.dumps(res, indent=2))\n",
    "\n",
    "# Quick plot\n",
    "plt.figure()\n",
    "plt.plot(ks, [r[\"P@k_mahal\"] for r in res], marker='o', label='Mahalanobis')\n",
    "plt.plot(ks, [r[\"P@k_lof\"]   for r in res], marker='o', label='LOF')\n",
    "plt.plot(ks, [r[\"P@k_fuse\"]  for r in res], marker='o', label='Fusion')\n",
    "plt.xlabel(\"k\"); plt.ylabel(\"Precision@k\"); plt.title(\"Precision@k on Blended Poison (10%)\")\n",
    "plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8a960c",
   "metadata": {},
   "source": [
    "## 9) Sanitize & retrain a student on cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf21320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sanitize_indices(top_order_indices, k):\n",
    "    return set(top_order_indices[:k])\n",
    "\n",
    "def retrain_student_on_cleaned(k=200, epochs=10, batch_size=64, num_workers=0):\n",
    "    removed = sanitize_indices(I[order_f], k)\n",
    "    transform = tv.transforms.Compose([tv.transforms.ToTensor()])\n",
    "    train = tv.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "    poisoned_set = set(poison_meta['indices'])\n",
    "    target_class = poison_meta['target_class']\n",
    "\n",
    "    class Cleaned(torch.utils.data.Dataset):\n",
    "        def __len__(self): return len(train)\n",
    "        def __getitem__(self, idx):\n",
    "            x,y = train[idx]\n",
    "            if idx in poisoned_set and idx in removed:\n",
    "                return None  # skip this sample\n",
    "            return x,y\n",
    "\n",
    "    def collate_skipnone(batch):\n",
    "        batch = [b for b in batch if b is not None]\n",
    "        xs, ys = zip(*batch)\n",
    "        return torch.stack(xs,0), torch.tensor(ys)\n",
    "\n",
    "    ds = Cleaned()\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_skipnone)\n",
    "\n",
    "    model = tv.models.resnet18(weights=None, num_classes=10).to(device)\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "    sched = optim.lr_scheduler.MultiStepLR(opt, milestones=[5, 8], gamma=0.1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total, correct, loss_sum = 0,0,0.0\n",
    "        for x,y in loader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            opt.zero_grad()\n",
    "            logits = model(x); loss = ce(logits,y)\n",
    "            loss.backward(); opt.step()\n",
    "            loss_sum += loss.item()*x.size(0)\n",
    "            pred = logits.argmax(1); correct += (pred==y).sum().item(); total += x.size(0)\n",
    "        sched.step()\n",
    "        if (epoch+1)%5==0:\n",
    "            print(f\"[student epoch {epoch+1}] loss={loss_sum/total:.4f} acc={(correct/total)*100:.2f}%\")\n",
    "    return model\n",
    "\n",
    "student = retrain_student_on_cleaned(k=200, epochs=10, batch_size=64, num_workers=0)\n",
    "print(\"Student clean acc:\", eval_clean_acc(student))\n",
    "print(\"Student ASR:\", eval_asr(student, target_class=poison_meta['target_class'], size=5, alpha=0.2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e802f0-159e-4a44-96b4-ccde40adb351",
   "metadata": {},
   "source": [
    "## 10) Experiment Summary: Metrics Snapshot → results/summary.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7c777a-fc3c-49a9-adde-0e9d32452eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summary = {\n",
    "    \"k_removed\": 200,\n",
    "    \"clean_acc_before\": float(clean_acc_poisoned),\n",
    "    \"asr_before\": float(asr_poisoned),\n",
    "    \"clean_acc_after\": float(eval_clean_acc(student)),\n",
    "    \"asr_after\": float(eval_asr(student, target_class=poison_meta['target_class'], size=5, alpha=0.2)),\n",
    "}\n",
    "import json, os\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "with open(\"results/summary.json\",\"w\") as f: json.dump(summary, f, indent=2)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7912d9-b6d3-48d3-b747-e73becdf2419",
   "metadata": {},
   "source": [
    "## 11) k-Sweep (50/100/200/400): Retrain & Measure Clean Acc / ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903ddeb8-27c6-444e-bbee-5a69cd11ab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sweep_k_and_measure(ks=(50,100,200,400), epochs=6):\n",
    "    rows = []\n",
    "    for k in ks:\n",
    "        print(f\"== k={k}\")\n",
    "        student_k = retrain_student_on_cleaned(k=k, epochs=epochs, batch_size=64, num_workers=0)\n",
    "        row = {\n",
    "            \"k\": k,\n",
    "            \"P@k_fuse\": precision_at_k(I[order_f], set(poison_meta[\"indices\"]), k),\n",
    "            \"clean_acc_after\": float(eval_clean_acc(student_k)),\n",
    "            \"asr_after\": float(eval_asr(student_k, target_class=poison_meta['target_class'], size=5, alpha=0.2)),\n",
    "        }\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "grid = sweep_k_and_measure(ks=(50,100,200,400), epochs=4)\n",
    "grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac4db1a-7df4-48dc-87ac-2268c863ceaf",
   "metadata": {},
   "source": [
    "## 12) Performance vs. Sanitization Strength (k-Sweep Plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d0be4f-a237-4ffa-adfd-17a79be11fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "ks = [r[\"k\"] for r in grid]\n",
    "asr = [r[\"asr_after\"] for r in grid]\n",
    "acc = [r[\"clean_acc_after\"] for r in grid]\n",
    "plt.plot(ks, asr, marker='o', label='ASR after sanitize'); \n",
    "plt.plot(ks, acc, marker='o', label='Clean Acc after sanitize'); \n",
    "plt.xlabel(\"k removed\"); plt.title(\"Sanitization effect vs k\"); plt.legend(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
